
```
# Title: "SPARK_WES QC Report"
# Author: "Nickie Safarian"
# Date: "2025-01-06"
# output:
  html_document: default
  pdf_document: default
```

```{r}
knitr::opts_chunk$set(echo = TRUE)
```

### **Background**

Quality control (QC) for Whole Exome Sequencing (WES) data relies on various metrics. 
Some of the metrics previously available in the GATK variant calling pipeline are no longer
included in the outputs generated by the newer GLNexus/joint calling pipeline.

This analysis compares QC outputs between the old and new versions of variant 
calling for the SPARK-WES1 cohort.

The comparison involves four data sets:

1. SPARK-WES1 (old version)
2. SPARK-WES1 (new version, i.e., the WES1 subset of SPARK-WES1to5 joint calls)
3. Elemi-shared Sub1 data
4. Elemi-shared Sub2 data


### **Step 1: Data_preparation**

I will first show the code I used to extract variants with freq_max <0.15 and 
typseq_priority class of Exonic| Exonic;splicing from chr22. 

#### *1.1. Prep the Old/GATK version of SPARK-WES1*

The path to the original data files is:

`/hpf/largeprojects/tcagstor/tcagstor_tmp/ebreetvelt/ASD_data/variants/SPARK_WES_1/`.

Note that this data is one data_file per sample, and there are 27,255 samples/data_files 
to process. For the purpose of simplicity, below I only show a one line code for one  data_file. 

```{bah}
#!/usr/bin/bash

#print the column_names to make sure of the columns number.
#column 226 is freq_max
#column 67 is typeseq_priority

zcat data_file.tsv.gz | awk -F $'\t' -v chr="22" 'NR == 1 || ($226 < 0.15 &&($67 ~/exonic/ || $67 ~/exonic;splicing/))' | gzip > ~/chr22_exonic_Old_data.tsv.gz 

```


#### *1.2. Prep the New/GLNexus version of SPARK-WES1*
This data to the original data is:

`/hpf/largeprojects/tcagstor/tcagstor_tmp/ebreetvelt/ASD_data/SPARK_WES_1-5_combined_calls+annotations/chr22/`

Note that this data is already split by chromosome, and every 300 samples are 
bundled into one data_file.

```{bash}

#!/usr/bin/bash

#print the column_names to make sure of the columns number.
#column 194 is freq_max
#column 35 is typeseq_priority

zcat data_file.tsv.gz | awk -F $'\t' 'NR == 1 || ($194 < 0.15 &&($35 ~/exonic/ || $35 ~/exonic;splicing/))' | gzip > ~/chr22_exonic_New_data.tsv.gz

```

### **Step 2: Analysis**

The size of the data are much reduced now and the rest of analysis can be 
performed in R.

#### load the packages
```{r}

library(tidyverse)
library(data.table)
library(readr)
library(knitr)

```


#### Import the metadata

The path to the SPARK-WES1 metadata is:

`/hpf/largeprojects/tcagstor/tcagstor_tmp/ebreetvelt/ASD_data/metadata/`

```{r, echo=FALSE}

# path:
metadata_dir <- "/hpf/largeprojects/tcagstor/tcagstor_tmp/ebreetvelt/ASD_data/metadata/"

# metadata:
wes1_meta <- readr::read_tsv(file.path(metadata_dir, "SPARK_WES_1_metadata.tsv"), col_names = TRUE)
wes2_meta <- readr::read_tsv(file.path(metadata_dir, "SPARK_WES_2_metadata.tsv"), col_names = TRUE)
wes3_meta <- readr::read_tsv(file.path(metadata_dir, "SPARK_WES_3_metadata.tsv"), col_names = TRUE)


# get the sample IDs
wes1_SampIDs <- wes1_meta$`Sample ID`#27,255
wes2_SampIDs <- wes2_meta$`Sample ID` #16004
wes3_SampIDs <- wes3_meta$`Sample ID` #16773

```


#### Import the preped SPARK-WES1 variants data (Chr22-LFvars-Exonic) 

I have concatenated all data_files from all samples into one single file, which
will be used here: 

```{r}

#path:
var_data_dir="hpf/largeprojects/tcagstor/tcagstor_tmp/nsafarian/Elemi_Projects/SPARK_WES_Project/Assess_QC_metrics/"

# old/GATK WES1 data:
old_spr1 <- readr::read_tsv(file.path(var_data_dir, "Old_GATK/processed_data/All_combined_Chr22_LFvars_ExonicSplicing.tsv.gz"), col_names = TRUE) 

dim(old_spr1)

```

```{r}

# new/ GLNexus WES1 data:
new_spr1 <- readr::read_tsv(file.path(var_data_dir,"New_GLNexus/processed_data/SPARK_WES1_chr22_LFvars_plus_HQcolumn_ExonicSplicing.tsv.gz"), col_names=TRUE)

```

```{r}

# Elemi-shared data
Elemi_sub1 <- readr::read_tsv(file.path(var_data_dir,"Elemi_shared_data/substraction1.csv.gz"), col_names=TRUE)

Elemi_sub2 <- readr::read_tsv(file.path(var_data_dir,"Elemi_shared_data/substraction2.csv.gz"), col_names=TRUE)

```


#### Comparisons:
I will compare the data sets at two levels: **A. sample-level**, **B. variants-level**.

##### **A. Sample-level comparisons**

```{r}

# I used the file_names as ID when I was concatenating data_files of the Old version. 
# Now I will extract the sample_IDs part from the full file_names:
head(old_spr1$ID)

old_spr1$SampleID <- gsub("\\.tsv.gz_filtered.tsv.gz_LFvar$", "", old_spr1$ID)
head(old_spr1$SampleID)

#################################################

#A.1. how many samples does each dataset have?
length(unique(old_spr1$SampleID))#27,255 

length(unique(new_spr1$SampleID))# 26,788

length(unique(Elemi_sub1$X.Sample))#27,255 

length(unique(Elemi_sub2$X.Sample_id)) #97,842

```


```{r}

#A.2. how many samples from each metadata?

#for old
length(intersect(old_spr1$SampleID, wes1_SampIDs ))#27,240
length(intersect(old_spr1$SampleID, wes2_SampIDs)) #49
length(intersect(old_spr1$SampleID, wes3_SampIDs)) #3

#for new
length(intersect(new_spr1$SampleID, wes1_SampIDs))#26,788
length(intersect(new_spr1$SampleID, wes2_SampIDs)) #49
length(intersect(new_spr1$SampleID, wes3_SampIDs)) #3

#for Elemi sub1 and sub2
length(intersect(Elemi_sub1$X.Sample, wes1_SampIDs))#27,255
length(intersect(Elemi_sub1$X.Sample, wes2_SampIDs)) #49
length(intersect(Elemi_sub1$X.Sample, wes3_SampIDs)) #3

length(intersect(Elemi_sub2$X.Sample_id, wes1_SampIDs))#24,512
length(intersect(Elemi_sub2$X.Sample_id, wes2_SampIDs)) #14,355
length(intersect(Elemi_sub2$X.Sample_id, wes3_SampIDs)) #15,063

```


```{r}

#A.3. how many samples overlap between datasets?

# plot the overlap
library(ggVennDiagram)
A=old_spr1$SampleID
B=new_spr1$SampleID
C=Elemi_sub1$X.Sample
D=Elemi_sub2$X.Sample_id

x=list(A, B, C,D) 
options(repr.plot.width=25, repr.plot.height=25)
ggVennDiagram(x, label_alpha = 0,label_offset = 0.5,font_size = 8,
              color=c("A" = "#4A4849" , "B" = "#E7872B" ,'C' = 'red3', 'D'= "#356920"),
              set_color = c("A" ="#4A4849","B" = "#E7872B" , 'C' = 'red3', 'D'= "#356920"),
              category.names = c("Old WES1\n(27,255)", "New WES1\n(26,788)",  "Elemi-Sub1\n(27,255)", "Elemi-Sub2\n(97,842)"))+
  scale_fill_gradient(low = "#F4FAFE55", high = "#4981BF")+
  scale_color_gradient(low = "#F4FAFE55", high = "#4981BF")+
  theme_void()+
  ggtitle("SampleID overlap between the datasets")+
  theme(plot.title = element_text(size=12, face="bold"),
        panel.spacing=unit(35, "lines"))

# get the common-sampleIDs
common_ids <- Reduce(intersect, list(A, B, C, D)) 
#24,499 SampleIDs are in common among the 4 datasets

```


```{r}

#A.4. subset all datasets to contain the same SampleIDs:

old_dt_sub <- old_spr1 %>% subset(SampleID%in% common_ids)
new_dt_sub <- new_spr1%>% subset(SampleID%in% common_ids)
Elemi_dt_sub1 <- Elemi_sub1%>% subset(X.Sample%in% common_ids)
Elemi_dt_sub2 <- Elemi_sub2%>% subset(X.Sample_id%in% common_ids)

```


##### **B. Variants-level comparisons**

The 4 datasets share the Identical Sample_IDs (n=24,499). Now, I'll make sure 
that variants in all 4 datasets are also the same in terms of:
1. Allele frequency (freq_max < 0.15)
2. Typeseq_priority (exonic or exonic;splicing)
3. Effect_priority (no synonymous variants included)

```{r}

#B.1. make sure the frequency of alleles are in the same range:
summary(old_dt_sub$freq_max) #max= 0.148 
summary(new_dt_sub$freq_max) #max=0.148
summary(Elemi_dt_sub1$freq_max) #max=0.95
summary(Elemi_dt_sub2$freq_max) #max=3,081


# subset the Elemi_sub1, for the freq_max<0.15
Elemi_dt_sub1 <- Elemi_dt_sub1 %>% subset(freq_max <0.15)

```


```{r}

# table(typeseq_priority) to see other classes, for instance "ncRNA_exonic" or
# "ncRNA_exonic;ncRNA_splicing" also present in the old/GATK data. 

#B.2. remove SNVs of any typeseq other than exonic
old_dt_sub <- old_dt_sub %>% subset(typeseq_priority %in% c("exonic", "exonic;splicing"))
new_dt_sub <- new_dt_sub %>% subset(typeseq_priority %in% c("exonic", "exonic;splicing"))
Elemi_dt_sub1 <- Elemi_dt_sub1 %>% subset(typeseq_priority %in% c("exonic", "exonic;splicing"))
Elemi_dt_sub2 <- Elemi_dt_sub2 %>% subset(typeseq_priority %in% c("exonic", "exonic;splicing"))

```


```{r}

#B.3. remove SNVs with effect type of synonymous and Unknown
old_dt_sub <- old_dt_sub %>% filter(! effect_priority %in% c("synonymous SNV", "unknown;UNKNOWN"))
new_dt_sub <- new_dt_sub %>% filter(! effect_priority %in% c("synonymous SNV", "unknown;UNKNOWN"))
Elemi_dt_sub1 <- Elemi_dt_sub1 %>% filter(! effect_priority %in% c("synonymous SNV", "unknown;UNKNOWN"))
Elemi_dt_sub2 <- Elemi_dt_sub2 %>% filter(! effect_priority %in% c("synonymous SNV", "unknown;UNKNOWN"))


```


```{r}

#B.4.creat unique snp id (recommended by Bhooma)
# here I append "gene_symbol" to "Annovar_key" to create the unique_varIDs. 

old_dt_sub <- old_dt_sub %>% mutate(unique_VarID = paste0(Annovar_key, "_", gene_symbol))
new_dt_sub <- new_dt_sub %>% mutate(unique_VarID= paste0(Annovar_key, "_", gene_symbol))
Elemi_dt_sub1 <- Elemi_dt_sub1 %>% mutate(unique_VarID= paste0(Annovar_key, "_", gene_symbol))
Elemi_dt_sub2 <- Elemi_dt_sub2 %>% mutate(unique_VarID= paste0(Annovar_key, "_", gene_symbol))


length(unique(old_dt_sub$unique_VarID)) 
length(unique(new_dt_sub$unique_VarID)) 
length(unique(Elemi_dt_sub1$unique_VarID)) 
length(unique(Elemi_dt_sub2$unique_VarID)) 

```


```{r}

#B.5. what are the count frequency for SNVs typeseq and effect?

#B.5.1. Define a function to create frequency tables 
get_counts <- function(data, column) {
  # Create the frequency table and convert to data frame
  counts <- as.data.frame(table(data[[column]]))
  # Rename the columns for clarity
  colnames(counts) <- c(column, "counts")
  return(counts)
}


#B.5.2. List  datasets
datasets <- list(old_dt_sub, new_dt_sub, Elemi_dt_sub1, Elemi_dt_sub2 )

#B.5.3. Columns for which to generate frequency tables
columns_to_count <- c("typeseq_priority", "effect_priority")

#B.5.4. Loop through datasets and columns 
counts <- lapply(datasets, function(data) {
  lapply(columns_to_count, function(col) {
    get_counts(data, col)
  })
})

#B.5.6. Naming the result
names(counts) <- paste0("Dataset_", seq_along(datasets))
for (i in seq_along(counts)) {
  names(counts[[i]]) <- columns_to_count
}

```

Now, the 4 datasets share:
1. Identical Sample_IDs (n=24,499),
2. The same allele frequency range (freq_max < 0.15),
3. The same variants’ typeseq_priority (exonic or exonic;splicing), and
4. No SNVs with effect_priority classified as synonymous or Unknown.

Note that these step could be done in bash, which is more efficient and faster. 
The only reason I preformed the above steps in R was to visualize the data in 
each step and fill the report. The updated bash code to run these steps would be:

```{bash}

zcat data_file.tsv.gz | awk -F $'\t' -v chr="22" 'NR == 1 || ($226 < 0.15 && ($67 ~/exonic/ || $67 ~/exonic;splicing/) && ($72 ~/nonsynonymous/ || $72 ~/start/ || $72 ~/frameshift/ || $72 ~/stop/))' | gzip > $output_dir/chr22_exonic_Old_data.tsv.gz


#column 226 is freq_max
#column 67 is typeseq_priority
#column 72 is effect_priority

```


##### **C. ASSESSING THE QC METRICS STEP_BY_STEP**

Now that the 4 datasets share:
1. Identical Sample_IDs (n=24,499),
2. The same allele frequency range (freq_max < 0.15),
3. The same variants’ typeseq_priority (exonic or exonic;splicing), and
4. No SNVs with effect_priority classified as synonymous or Unknown.

I will conduct a step-by-step quality control (QC) analysis to identify the 
stages where the SNVs count discrepancies occur.

###### **Old QC metrics are:**
1. QUAL >= 30
2. FS <=60
3. ReadPosRankSum	>=-8
4. RMSMappingQuality (MQ)	>=40
5. StrandOddsRatio (SOR)	>3
6. MappingQualityRankSumTest (MQRankSum)	>-12.5
7. gnomAD_exome211_FILTER =='PASS'

###### **New QC metrics are:**
1. QUAL >=30
2. Sample:GQ>20
3. Sample:DP >10
4. gnomAD_exome211_FILTER =='PASS'

Note:  I used  the HQ column to call in the high quality variants in the New/GLNexus version.


###### **C.1. For the Old-GATK WES1 data:**
```{r}

# metrics:
#1.
old_dt_sub.1 <- old_dt_sub%>% subset(QUAL >= 30)
#2.
old_dt_sub.2 <- old_dt_sub.1%>% subset(FS <=60)
#3.
old_dt_sub.3 <- old_dt_sub.2%>% subset(ReadPosRankSum>= -8)


# The remaining criteria are applied on only non-synonymous types
#4. MQ 
old_dt_sub.3 <- old_dt_sub.3 %>%
  mutate(MQ_filter = ifelse(effect_priority == "nonsynonymous SNV" & MQ < 40, NA, MQ))


old_dt_sub.4 <- old_dt_sub.3 %>% filter(!is.na(MQ_filter))    


#5. MQRankSum
old_dt_sub.4 <- old_dt_sub.4 %>%
  mutate(MQRank_filter = ifelse(effect_priority == "nonsynonymous SNV" & MQRankSum < -12.5, NA, MQRankSum))

old_dt_sub.5 <- old_dt_sub.4 %>% filter(!is.na(MQRank_filter))


#6. SOR
old_dt_sub.5 <- old_dt_sub.5 %>%
  mutate(SOR_filter = ifelse(effect_priority == "nonsynonymous SNV" & SOR >3, NA, SOR))

old_dt_sub.6 <- old_dt_sub.5 %>% filter(!is.na(SOR_filter))


#7. gnomAD_FILTER
table(old_dt_sub.6$gnomAD_exome211_FILTER)

old_dt_sub.7 <- old_dt_sub.6 %>% filter(gnomAD_exome211_FILTER =="PASS")

```


```{r}

# Get the extent of count reduction in each step:

# List the data frames
old_sub_versions <- c("old_dt_sub.1", "old_dt_sub.2", "old_dt_sub.3", 
                      "old_dt_sub.4", "old_dt_sub.5", 
                      "old_dt_sub.6","old_dt_sub.7")
                  

# Iterate over each df
for (df in old_sub_versions) {
  # Access the data frame dynamically
  current_df <- get(df)
  
  # Calculate and print length of unique VarID
  uniq_varid_length <- length(unique(current_df$unique_VarID))
  cat(df, "unique_VarID length:", unique_varid_length, "\n")
  
  # Calculate and print table of typeseq_priority
  typeseq_table <- table(current_df$typeseq_priority)
  cat(df, "typeseq_priority table:\n")
  print(typeseq_priority_table)
  cat("\n")
}


```


###### **C.2. For the New_GLNexus WES1 data:**
```{r}

# metrics:
#1. QUAL
new_dt_sub.1 <- new_dt_sub%>% subset(QUAL >= 30)

#2.Sample:GQ and Sample:DP

new_dt_sub.1.HQ <- new_dt_sub.1 %>% filter(high_quality=="TRUE")

#3. gnomAD_exome211_FILTER
table(new_dt_sub.1$gnomAD_exome211_FILTER)

new_dt_sub.1.HQ.1 <- new_dt_sub.1.HQ %>% filter(gnomAD_exome211_FILTER=="PASS")

```


```{r}

# Get the extent of count reduction in each step:

# List the data frames
new_sub_versions <- c("new_dt_sub", "new_dt_sub.1", 
                      "new_dt_sub.1.HQ", "new_dt_sub.1.HQ.1")
                      
                  

# Iterate over each df
for (df in new_sub_versions) {
  # Access the data frame dynamically
  current_df <- get(df)
  
  # Calculate and print length of unique VarID
  uniq_varid_length <- length(unique(current_df$unique_VarID))
  cat(df, "unique_VarID length:", unique_varid_length, "\n")
  
  # Calculate and print table of typeseq_priority
  typeseq_table <- table(current_df$typeseq_priority)
  cat(df, "typeseq_priority table:\n")
  print(typeseq_priority_table)
  cat("\n")
}

```


```{r}

library(cowplot)

# plot Sample:DP (before and after filtering)
p1 <- new_dt_sub.1%>%
  ggplot(aes(x=`Sample:DP`))+
  geom_density(size=1, color= '#28827A') +  
  ggtitle("new_SPR1 `Sample:DP`\nBefore Filtering")+
  theme_bw()+
  scale_x_continuous(breaks=c(0, 500, 1000, 1500))+
  scale_y_continuous(breaks=c(0, 0.005, 0.010, 0.015))+
  theme(axis.text = element_text(face="bold", size=11, color="#1B1B1B"),
        plot.title = element_text(face="bold", size=9, color="#1B1B1B"),
        axis.title.y = element_text(vjust = 3, face="bold", size=12, color="#1B1B1B"),
        axis.title.x = element_text(vjust = -3, face="bold", size=12, color="#1B1B1B"),
        legend.position = "none",
        panel.spacing=unit(30,"pt"),
        plot.margin = margin(1,1,1.5,1.2, "cm")) 

p2 <- new_dt_sub.1.HQ.1%>%
  ggplot(aes(x=`Sample:DP`))+
  geom_density(size=1, color='#87C0BC') + 
  ggtitle("new_SPR1_HQ `Sample:DP`\nAfter Filtering for HQ vars")+
  theme_bw()+
  scale_x_continuous(breaks=c(0, 500,1000, 1500))+
  scale_y_continuous(breaks=c(0, 0.005, 0.010, 0.015))+
  #scale_y_continuous(breaks=c(0, 5, 10, 15,20, 25, 30))+
  theme(axis.text = element_text(face="bold", size=11, color="#1B1B1B"),
        plot.title = element_text(face="bold", size=9, color="#1B1B1B"),
        axis.title.y = element_text(vjust = 3, face="bold", size=12, color="#1B1B1B"),
        axis.title.x = element_text(vjust = -3, face="bold", size=12, color="#1B1B1B"),
        legend.position = "none",
        panel.spacing=unit(30,"pt"),
        plot.margin = margin(1,1,1.5,1.2, "cm")) 

plot_grid(p1, p2)

```


```{r}

library(viridis)
# use scale_color_viridis_d() instead if you prefer. 

p3 <- new_dt_sub.1%>%
  ggplot(aes(x=`Sample:GQ`, group=`Sample:GT`, color= `Sample:GT`))+
  geom_density(size=1) +  
  ggtitle("new_SPR1 `Sample:GQ`\nBefore Filtering")+
  theme_bw()+ 
  scale_color_manual(values = c("#2D5F8A","#C84D4C", "#28827A",'#B33D90'))+
  scale_x_continuous(breaks=c(90, 92, 94,96, 98, 100, 102))+
  scale_y_continuous(breaks=c(0, 5, 10, 15, 20))+
  guides(color = guide_legend(ncol = 2))+
  theme(axis.text = element_text(face="bold", size=11, color="#1B1B1B"),
        plot.title = element_text(face="bold", size=9, color="#1B1B1B"),
        axis.title.y = element_text(vjust = 3, face="bold", size=12, color="#1B1B1B"),
        axis.title.x = element_text(vjust = -3, face="bold", size=12, color="#1B1B1B"),
        legend.position = "bottom",
        legend.text = element_text(size = 9, face = "italic", color = "#1B1B1B"),
        legend.title = element_text(size = 10, face = "bold", color = "#1B1B1B"),
        panel.spacing=unit(30,"pt"),
        plot.margin = margin(1,1,1.5,1.2, "cm")) 

p4 <- new_dt_sub.1.HQ.1 %>%
  ggplot(aes(x=`Sample:GQ`, group=`Sample:GT`, color= `Sample:GT`))+
  geom_density(size=1) + 
  ggtitle("new_SPR1_HQ `Sample:GQ`\nAfter Filtering for HQ vars")+
  theme_bw()+
  scale_color_manual(values = c("#2D5F8A","#C84D4C", '#237A43','#e3a002' ))+
  scale_x_continuous(breaks = c(90, 92, 94,96, 98, 100, 102))+
  scale_y_continuous(breaks=c(0, 100, 200, 300, 400))+
  guides(color = guide_legend(ncol = 2))+
  theme(axis.text = element_text(face="bold", size=11, color="#1B1B1B"),
        plot.title = element_text(face="bold", size=9, color="#1B1B1B"),
        axis.title.y = element_text(vjust = 3, face="bold", size=12, color="#1B1B1B"),
        axis.title.x = element_text(vjust = -3, face="bold", size=12, color="#1B1B1B"),
        legend.position = "bottom",
        legend.text = element_text(size = 9, face = "italic", color = "#1B1B1B"),
        legend.title = element_text(size = 10, face = "bold", color = "#1B1B1B"),
        panel.spacing=unit(30,"pt"),
        plot.margin = margin(1,1,1.5,1.2, "cm")) 

plot_grid(p3, p4)

```


###### **C.3. For the Elemi-sub1 data:**

If I run the QC steps (as outlined in section **C.1**) on the Elemi-sub1 data, the output contains 1,874 unique varIDs. This significant difference between my results and Elemi's data prompted me to examine the position (POS) of the variants. Below are the results of this analysis:

```{r}

# check on POS column:

summary(Elemi_sub1$POS)
# Min.       1st Qu.   Median     Mean     3rd Qu.     Max. 
# 18,528,267 19210439  19447481  19600664 20037073   20,395,411 


# subset the old and the new data for the same coords:

old_final <- old_dt_sub.7 %>% dplyr::filter(POS >= 18528267 & POS <= 20395411)

new_final <- new_dt_sub.1.HQ.1 %>% dplyr::filter(POS >= 18528267 & POS <= 20395411)



length(intersect(unique(old_dt_sub.7$unique_VarID), 
                 unique(Elemi_sub1$unique_VarID) )) #1874


length(intersect(unique(new_dt_sub.1.HQ.1$unique_VarID), 
                 unique(Elemi_sub$unique_VarID) ))#1776  


```

***Horray!!!***
